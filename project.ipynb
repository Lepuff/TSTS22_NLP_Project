{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def prep_input(series):\n",
    "    nlp = spacy.load(\"sv_core_news_sm\")\n",
    "    \n",
    "\n",
    "    regexp4 = r'\\b(?<![\\\\\\.])\\w+(?!\\.\\w+)\\b'\n",
    "    result_list = []\n",
    "    token_list = []\n",
    "    for row in nlp.pipe(series):\n",
    "        res = BeautifulSoup(row.text, 'lxml').get_text()\n",
    "        res = re.findall(regexp4, res)\n",
    "        token_list.append([token.lemma_.lower() for token in row if not token.is_stop \n",
    "                                                       and token.is_alpha])\n",
    "    \n",
    "    #display(pd.Series(token_list))\n",
    "    return pd.Series(token_list)\n",
    "\n",
    "def preprocess_input(input):\n",
    "    nlp = spacy.load(\"sv_core_news_sm\")\n",
    "\n",
    "    regexp4 = r'\\b(?<![\\\\\\.])\\w+(?!\\.\\w+)\\b'\n",
    "    \n",
    "\n",
    "    token_list = []\n",
    "    # Remove unicode and webpages\n",
    "    \n",
    "    # Remove html tags, 'lxml' more robust than 'html.parser'\n",
    "    # Requires pip install lxml\n",
    "    result = BeautifulSoup(input, 'lxml').get_text()\n",
    "    \n",
    "    # Save words that are not preceded by a '\\' and webpages\n",
    "    result = re.findall(regexp4, result)\n",
    "    \n",
    "\n",
    "    # Remove excessive whitespaces\n",
    "    result = \" \".join(result)\n",
    "    doc = nlp.pipe([result])\n",
    "    for token in doc:\n",
    "        token_list.append([t.lemma_.lower() for t in token if not t.is_stop \n",
    "                                                    and t.is_alpha])\n",
    "    result = \" \".join(token_list[0])\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aktiespararna gunvorbudet\n",
      "fast timme grop\n",
      "mannen fastna grop timme\n",
      "motorcykel\n",
      "stulna braskamin värma\n",
      "sävsjö ny skolchef\n",
      "krösatåg försena\n",
      "sävsjöpensionär uppra\n",
      "linus testa\n",
      "linus testa\n",
      "linus test\n",
      "test\n",
      "linus\n",
      "test\n",
      "testing\n",
      "dagisinbrott vecka\n",
      "sävjöborna köra bil\n",
      "liter diesel stängsl\n",
      "tyskt teleföretag bluffakturera vrigstad\n",
      "tuffa brud lyxförpackning\n",
      "julskyltning val lucia\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\strep\\courses\\NLP\\TSTS22_NLP_Project\\project.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/strep/courses/NLP/TSTS22_NLP_Project/project.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m df_articles\u001b[39m.\u001b[39mcolumns[\u001b[39m1\u001b[39m:]:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/strep/courses/NLP/TSTS22_NLP_Project/project.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         df_articles[col] \u001b[39m=\u001b[39m df_articles[col]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/strep/courses/NLP/TSTS22_NLP_Project/project.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         df_articles[col] \u001b[39m=\u001b[39m df_articles[col]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: preprocess_input(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/strep/courses/NLP/TSTS22_NLP_Project/project.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39m#df_articles[col] = prep_input(df_articles[col])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/strep/courses/NLP/TSTS22_NLP_Project/project.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39m#df_articles[col] = preprocess_input(df_articles[col])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/strep/courses/NLP/TSTS22_NLP_Project/project.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/strep/courses/NLP/TSTS22_NLP_Project/project.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/strep/courses/NLP/TSTS22_NLP_Project/project.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     df_processed \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mpreprocessed_articles.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\strep\\miniconda3\\envs\\nlp\\lib\\site-packages\\pandas\\core\\series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4324\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4328\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4329\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4330\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4431\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4432\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4433\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\strep\\miniconda3\\envs\\nlp\\lib\\site-packages\\pandas\\core\\apply.py:1088\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1084\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1085\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\strep\\miniconda3\\envs\\nlp\\lib\\site-packages\\pandas\\core\\apply.py:1143\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1137\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m   1138\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1143\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1144\u001b[0m             values,\n\u001b[0;32m   1145\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1146\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1147\u001b[0m         )\n\u001b[0;32m   1149\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1150\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1151\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1152\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\strep\\miniconda3\\envs\\nlp\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\strep\\courses\\NLP\\TSTS22_NLP_Project\\project.ipynb Cell 3\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/strep/courses/NLP/TSTS22_NLP_Project/project.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m df_articles\u001b[39m.\u001b[39mcolumns[\u001b[39m1\u001b[39m:]:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/strep/courses/NLP/TSTS22_NLP_Project/project.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         df_articles[col] \u001b[39m=\u001b[39m df_articles[col]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/strep/courses/NLP/TSTS22_NLP_Project/project.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         df_articles[col] \u001b[39m=\u001b[39m df_articles[col]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: preprocess_input(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/strep/courses/NLP/TSTS22_NLP_Project/project.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39m#df_articles[col] = prep_input(df_articles[col])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/strep/courses/NLP/TSTS22_NLP_Project/project.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39m#df_articles[col] = preprocess_input(df_articles[col])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/strep/courses/NLP/TSTS22_NLP_Project/project.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/strep/courses/NLP/TSTS22_NLP_Project/project.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/strep/courses/NLP/TSTS22_NLP_Project/project.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     df_processed \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mpreprocessed_articles.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\strep\\courses\\NLP\\TSTS22_NLP_Project\\project.ipynb Cell 3\u001b[0m in \u001b[0;36mpreprocess_input\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/strep/courses/NLP/TSTS22_NLP_Project/project.ipynb#W2sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_input\u001b[39m(\u001b[39minput\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/strep/courses/NLP/TSTS22_NLP_Project/project.ipynb#W2sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39msv_core_news_sm\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/strep/courses/NLP/TSTS22_NLP_Project/project.ipynb#W2sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     regexp4 \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mb(?<![\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\\\u001b[39m\u001b[39m.])\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+(?!\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+)\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/strep/courses/NLP/TSTS22_NLP_Project/project.ipynb#W2sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     token_list \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\strep\\miniconda3\\envs\\nlp\\lib\\site-packages\\spacy\\__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[0;32m     31\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[0;32m     32\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     38\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[0;32m     39\u001b[0m     \u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[0;32m     55\u001b[0m         name,\n\u001b[0;32m     56\u001b[0m         vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[0;32m     57\u001b[0m         disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[0;32m     58\u001b[0m         enable\u001b[39m=\u001b[39;49menable,\n\u001b[0;32m     59\u001b[0m         exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[0;32m     60\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m     61\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\strep\\miniconda3\\envs\\nlp\\lib\\site-packages\\spacy\\util.py:429\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[39mreturn\u001b[39;00m get_lang_class(name\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mblank:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))()\n\u001b[0;32m    428\u001b[0m \u001b[39mif\u001b[39;00m is_package(name):  \u001b[39m# installed as package\u001b[39;00m\n\u001b[1;32m--> 429\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_package(name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[39mif\u001b[39;00m Path(name)\u001b[39m.\u001b[39mexists():  \u001b[39m# path to model data directory\u001b[39;00m\n\u001b[0;32m    431\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_path(Path(name), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\strep\\miniconda3\\envs\\nlp\\lib\\site-packages\\spacy\\util.py:465\u001b[0m, in \u001b[0;36mload_model_from_package\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[39m\"\"\"Load a model from an installed package.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \n\u001b[0;32m    450\u001b[0m \u001b[39mname (str): The package name.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mRETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m    463\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mimport_module(name)\n\u001b[1;32m--> 465\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mload(vocab\u001b[39m=\u001b[39;49mvocab, disable\u001b[39m=\u001b[39;49mdisable, enable\u001b[39m=\u001b[39;49menable, exclude\u001b[39m=\u001b[39;49mexclude, config\u001b[39m=\u001b[39;49mconfig)\n",
      "File \u001b[1;32mc:\\Users\\strep\\miniconda3\\envs\\nlp\\lib\\site-packages\\sv_core_news_sm\\__init__.py:10\u001b[0m, in \u001b[0;36mload\u001b[1;34m(**overrides)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moverrides):\n\u001b[1;32m---> 10\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_init_py(\u001b[39m__file__\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moverrides)\n",
      "File \u001b[1;32mc:\\Users\\strep\\miniconda3\\envs\\nlp\\lib\\site-packages\\spacy\\util.py:646\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[1;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_path\u001b[39m.\u001b[39mexists():\n\u001b[0;32m    645\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE052\u001b[39m.\u001b[39mformat(path\u001b[39m=\u001b[39mdata_path))\n\u001b[1;32m--> 646\u001b[0m \u001b[39mreturn\u001b[39;00m load_model_from_path(\n\u001b[0;32m    647\u001b[0m     data_path,\n\u001b[0;32m    648\u001b[0m     vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[0;32m    649\u001b[0m     meta\u001b[39m=\u001b[39;49mmeta,\n\u001b[0;32m    650\u001b[0m     disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[0;32m    651\u001b[0m     enable\u001b[39m=\u001b[39;49menable,\n\u001b[0;32m    652\u001b[0m     exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[0;32m    653\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m    654\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\strep\\miniconda3\\envs\\nlp\\lib\\site-packages\\spacy\\util.py:511\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    502\u001b[0m config \u001b[39m=\u001b[39m load_config(config_path, overrides\u001b[39m=\u001b[39moverrides)\n\u001b[0;32m    503\u001b[0m nlp \u001b[39m=\u001b[39m load_model_from_config(\n\u001b[0;32m    504\u001b[0m     config,\n\u001b[0;32m    505\u001b[0m     vocab\u001b[39m=\u001b[39mvocab,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m     meta\u001b[39m=\u001b[39mmeta,\n\u001b[0;32m    510\u001b[0m )\n\u001b[1;32m--> 511\u001b[0m \u001b[39mreturn\u001b[39;00m nlp\u001b[39m.\u001b[39;49mfrom_disk(model_path, exclude\u001b[39m=\u001b[39;49mexclude, overrides\u001b[39m=\u001b[39;49moverrides)\n",
      "File \u001b[1;32mc:\\Users\\strep\\miniconda3\\envs\\nlp\\lib\\site-packages\\spacy\\language.py:2115\u001b[0m, in \u001b[0;36mLanguage.from_disk\u001b[1;34m(self, path, exclude, overrides)\u001b[0m\n\u001b[0;32m   2112\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (path \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mexists() \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m exclude:  \u001b[39m# type: ignore[operator]\u001b[39;00m\n\u001b[0;32m   2113\u001b[0m     \u001b[39m# Convert to list here in case exclude is (default) tuple\u001b[39;00m\n\u001b[0;32m   2114\u001b[0m     exclude \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(exclude) \u001b[39m+\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m-> 2115\u001b[0m util\u001b[39m.\u001b[39;49mfrom_disk(path, deserializers, exclude)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   2116\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path \u001b[39m=\u001b[39m path  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m   2117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_link_components()\n",
      "File \u001b[1;32mc:\\Users\\strep\\miniconda3\\envs\\nlp\\lib\\site-packages\\spacy\\util.py:1340\u001b[0m, in \u001b[0;36mfrom_disk\u001b[1;34m(path, readers, exclude)\u001b[0m\n\u001b[0;32m   1337\u001b[0m \u001b[39mfor\u001b[39;00m key, reader \u001b[39min\u001b[39;00m readers\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   1338\u001b[0m     \u001b[39m# Split to support file names like meta.json\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m     \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m exclude:\n\u001b[1;32m-> 1340\u001b[0m         reader(path \u001b[39m/\u001b[39;49m key)\n\u001b[0;32m   1341\u001b[0m \u001b[39mreturn\u001b[39;00m path\n",
      "File \u001b[1;32mc:\\Users\\strep\\miniconda3\\envs\\nlp\\lib\\site-packages\\spacy\\language.py:2091\u001b[0m, in \u001b[0;36mLanguage.from_disk.<locals>.deserialize_vocab\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m   2089\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeserialize_vocab\u001b[39m(path: Path) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2090\u001b[0m     \u001b[39mif\u001b[39;00m path\u001b[39m.\u001b[39mexists():\n\u001b[1;32m-> 2091\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39;49mfrom_disk(path, exclude\u001b[39m=\u001b[39;49mexclude)\n",
      "File \u001b[1;32mc:\\Users\\strep\\miniconda3\\envs\\nlp\\lib\\site-packages\\spacy\\vocab.pyx:489\u001b[0m, in \u001b[0;36mspacy.vocab.Vocab.from_disk\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\strep\\miniconda3\\envs\\nlp\\lib\\site-packages\\spacy\\strings.pyx:267\u001b[0m, in \u001b[0;36mspacy.strings.StringStore.from_disk\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\strep\\miniconda3\\envs\\nlp\\lib\\site-packages\\srsly\\_json_api.py:52\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[39mreturn\u001b[39;00m ujson\u001b[39m.\u001b[39mloads(data)\n\u001b[0;32m     51\u001b[0m file_path \u001b[39m=\u001b[39m force_path(path)\n\u001b[1;32m---> 52\u001b[0m \u001b[39mwith\u001b[39;00m file_path\u001b[39m.\u001b[39mopen(\u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     53\u001b[0m     \u001b[39mreturn\u001b[39;00m ujson\u001b[39m.\u001b[39mload(f)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from os.path import exists\n",
    "\n",
    "if(not exists('preprocessed_articles.csv')):  \n",
    "    df_articles = pd.read_csv('sävsjö_articles.csv')\n",
    "    df_articles = df_articles.dropna().reset_index(drop=True)\n",
    "    # columns: ['ID', 'Title', 'Text']\n",
    "    # iterate over 'Title' and 'Text' columns and preprocess the texts\n",
    "    for col in df_articles.columns[1:]:\n",
    "        df_articles[col] = df_articles[col].astype(str)\n",
    "        df_articles[col] = df_articles[col].apply(lambda x: preprocess_input(x))\n",
    "        #df_articles[col] = prep_input(df_articles[col])\n",
    "        #df_articles[col] = preprocess_input(df_articles[col])\n",
    "    \n",
    "else:\n",
    "    df_processed = pd.read_csv('preprocessed_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_labels(doc, labels:list):\n",
    "        text_no_locations = ''\n",
    "        for token in doc:\n",
    "            if token.ent_type_ not in labels:\n",
    "                text_no_locations += token.text\n",
    "                if token.whitespace_:\n",
    "                    text_no_locations += ' '\n",
    "        return text_no_locations\n",
    "\n",
    "def retrieve_tokens(doc):\n",
    "    return [token.lemma_.lower() for token in doc if not token.is_stop \n",
    "                                                     and token.is_alpha]\n",
    "    #return [token.lemma_.lower() for token in doc if token.pos_ not in pos_to_remove \n",
    "    #                                                                    and not token.is_stop \n",
    "    #                                                                    and token.is_alpha \n",
    "    #                                                                    and token.ent_type_ not in labels_to_remove]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from spacy.pipeline.ner import EntityRecognizer\n",
    "from matplotlib.pyplot import text\n",
    "from numpy import dtype\n",
    "import spacy\n",
    "from os.path import exists\n",
    "np.warnings.filterwarnings('error', category=np.VisibleDeprecationWarning) \n",
    "\n",
    "if(not exists('preprocessed_articles.csv')):\n",
    "    df_processed = df_articles.drop(columns=['Title']).copy()\n",
    "\n",
    "    nlp = spacy.load(\"sv_core_news_sm\")\n",
    "    labels_to_remove = ['LOC', 'TME', 'MSR']\n",
    "    pos_to_remove = ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE', 'NUM', 'SYM']\n",
    "    \n",
    "    entity_list = []\n",
    "    label_list = []\n",
    "    unique_token_list = []\n",
    "    token_list = []\n",
    "    \"\"\"\n",
    "    for row in nlp.pipe(df_processed['Text']):\n",
    "        label_free_text = remove_labels(row, labels_to_remove)\n",
    "        row_tokens = [token.lemma_.lower() for token in row if token.pos_ not in pos_to_remove \n",
    "                                                                            and not token.is_stop \n",
    "                                                                            and token.is_alpha \n",
    "                                                                            and token.ent_type_ not in labels_to_remove]\n",
    "        \n",
    "        unique_ents = list({keyword.__repr__(): keyword for keyword in row.ents}.values())\n",
    "        unique_ents = [ent for ent in unique_ents if not ent.label_ in labels_to_remove]\n",
    "        entities = [str(x) for x in unique_ents]\n",
    "        \n",
    "        labels = [str(labels.label_) for labels in unique_ents]\n",
    "        entity_list.append(entities)\n",
    "        label_list.append(labels)\n",
    "        tokens.append(row_tokens)\n",
    "        text_list.append(label_free_text)\n",
    "        #print(unique_ents)\n",
    "        \n",
    "    df_processed['Entities'] = pd.Series(entity_list)\n",
    "    df_processed['Labels'] = pd.Series(label_list)\n",
    "    df_processed['Tokens'] = pd.Series(tokens)\n",
    "    df_processed['Text'] = pd.Series(text_list)\n",
    "    \"\"\"\n",
    "\n",
    "    for index, row in df_articles.iterrows():\n",
    "        \n",
    "        doc_text = nlp(row['Text'])\n",
    "        doc_title = nlp(row['Title'])\n",
    "        \n",
    "        doc_ents = doc_text.ents + doc_title.ents\n",
    "        #text_tokens = retrieve_tokens(doc_text, pos_to_remove, labels_to_remove)\n",
    "        #title_tokens = retrieve_tokens(doc_title, pos_to_remove, labels_to_remove)\n",
    "        print(doc_text)\n",
    "        title_tokens = retrieve_tokens(doc_title)\n",
    "        text_tokens = retrieve_tokens(doc_text)\n",
    "        print(text_tokens)\n",
    "        doc_tokens = title_tokens + text_tokens\n",
    "        unique_doc_tokens = set(doc_tokens)\n",
    "        # Remove duplicate entities from the list\n",
    "        unique_ents = list({keyword.__repr__(): keyword for keyword in doc_ents}.values())\n",
    "        \n",
    "        #entity_list.add(str(unique_ents))\n",
    "        # Remove entities that have label 'TME' or 'LOC' because they are mostly redundant\n",
    "        # And the text will be changed to not contain words with these labels\n",
    "        unique_ents = [ent for ent in unique_ents if not ent.label_ in labels_to_remove]    \n",
    "           \n",
    "        entities = [str(x) for x in unique_ents]\n",
    "        labels = [str(labels.label_) for labels in unique_ents]\n",
    "        \n",
    "        # Remove words that contain the specified labels\n",
    "        label_free_text = remove_labels(doc_text, labels=labels_to_remove)\n",
    "\n",
    "        entity_list.append(entities)\n",
    "        label_list.append(labels)\n",
    "        token_list.append(doc_tokens)\n",
    "        unique_token_list.append(unique_doc_tokens)\n",
    "        \n",
    "        df_processed.at[index,['Text']] = label_free_text\n",
    "        \n",
    "        if index > 1:\n",
    "            break\n",
    "        \n",
    "    df_processed['Entities'] = pd.Series(entity_list)\n",
    "    df_processed['Labels'] = pd.Series(label_list)\n",
    "    df_processed['Tokens'] = pd.Series(token_list)\n",
    "    df_processed['Unique_Tokens'] = pd.Series(unique_token_list)\n",
    "    #df_processed.to_csv('preprocessed_articles.csv')\n",
    "\n",
    "df_processed.head(10)\n",
    "\n",
    "# pipeline takes 6m 11s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed['Text'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove empty rows and convert to lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not used so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Check for a value in a dataframe column\"\"\"\n",
    "def check_value(df, column, value):\n",
    "    return df[df[column].str.contains(value, na=False)]\n",
    "check_value(df_processed, 'Entities', 'Smålandsvillan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "df_test = df_articles[[\"Labels\",\"Entities\"]]\n",
    "\n",
    "for ind, row in df_test.iterrows():\n",
    "    if row[\"Labels\"] == \"[]\":\n",
    "        df_test = df_test.drop(index=ind)\n",
    "\n",
    "df_test[[\"Labels\",\"Entities\"]].apply(lambda x : str(x).split(','))\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73e735a07ce367303aaadeabd025808cbfe97e5b002cadee1be2f1eb67320e65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
