{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_input(input):\n",
    "    #regexp = r'[\\\\\\/\\.,;:()]+\\w*'\n",
    "    #regexp2 = r'\\b(?<![\\\\\\/\\.\\,\\\\:\\;\\(\\)\\+\\-])\\w+\\b'\n",
    "    #regexp3 = r'\\b(?<![\\\\\\/])\\w+\\.*\\w*\\b'\n",
    "\n",
    "    # Remove unicode and webpages\n",
    "    regexp4 = r'\\b(?<![\\\\\\.])\\w+(?!\\.\\w+)\\b'\n",
    "\n",
    "    # Regex for removing words that are preceded by '\\'\n",
    "    unicode_regexp = r'\\b(?<![\\\\])\\w+\\b'\n",
    "\n",
    "    # Regex for removing webpages\n",
    "    webpage_regexp = r'\\b(?<![\\.])\\w+(?!\\.\\w+)\\b'\n",
    "\n",
    "    # Remove html tags\n",
    "    result = BeautifulSoup(input, 'lxml').get_text()\n",
    "    \n",
    "\n",
    "\n",
    "    # Save words that are not preceded by a '\\' and webpages\n",
    "    result = re.findall(regexp4, result)\n",
    "    \n",
    "            # TRYING TO COMBINE MULTIPLE REGEX, WORK IN PROGRESS\n",
    "            #patterns = [unicode_regexp, webpage_regexp]\n",
    "            #pattern = \"|\".join(patterns)  # pattern1|pattern2|pattern3|...\n",
    "            #re.findall(pattern, result)\n",
    "\n",
    "    # Remove excessive whitespaces\n",
    "    result = \" \".join(result)\n",
    "\n",
    "\n",
    "    #result = \" \".join(result.split())\n",
    "   \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "\n",
    "if(not exists('preprocessed_articles.csv')):  \n",
    "    df_articles = pd.read_csv('sävsjö_articles.csv')\n",
    "    df_articles = df_articles.dropna()\n",
    "    # columns: ['ID', 'Title', 'Text']\n",
    "    # iterate over 'Title' and 'Text' columns and preprocess the texts\n",
    "    for col in df_articles.columns[1:]:\n",
    "        df_articles[col] = df_articles[col].astype(str)\n",
    "        df_articles[col] = df_articles[col].apply(lambda x: preprocess_input(x))\n",
    "    \n",
    "else:\n",
    "    df_articles = pd.read_csv('preprocessed_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from spacy.pipeline.ner import EntityRecognizer\n",
    "import spacy\n",
    "from os.path import exists\n",
    "\n",
    "if(not exists('preprocessed_articles.csv')):   \n",
    "\n",
    "    nlp = spacy.load(\"sv_core_news_sm\")\n",
    "    df_articles['Entities'] = '0'\n",
    "    df_articles['Labels'] = '0'\n",
    "\n",
    "    for index, row in df_articles.iterrows():\n",
    "        doc_text = nlp(row['Text'])\n",
    "        doc_title = nlp(row['Title'])\n",
    "    \n",
    "        doc_ents = doc_text.ents + doc_title.ents\n",
    "\n",
    "        # Remove duplicate entities from the list\n",
    "        unique_ents = list({keyword.__repr__(): keyword for keyword in doc_ents}.values())\n",
    "\n",
    "        # Remove entities that have label 'TME' because the words are mostly redundant\n",
    "        unique_ents = [ent for ent in unique_ents if not ent.label_ in ['TME']]        \n",
    "        \n",
    "        entities = [str(x) for x in unique_ents]\n",
    "        labels = [str(labels.label_) for labels in unique_ents]\n",
    "        \n",
    "        df_articles.loc[index,['Entities', 'Labels']] = f'{entities}', f'{labels}'\n",
    "        if index > 10:\n",
    "            break\n",
    "    df_articles.to_csv('preprocessed_articles.csv')\n",
    "\n",
    "df_articles.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove empty rows and convert to lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_articles[[\"Labels\",\"Entities\"]]\n",
    "\n",
    "for ind, row in df_test.iterrows():\n",
    "    if row[\"Labels\"] == \"[]\":\n",
    "        df_test = df_test.drop(index=ind)\n",
    "\n",
    "df_test[[\"Labels\",\"Entities\"]].apply(lambda x : str(x).split(','))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "882a6b7fc3ca80e688227decd54209862062b8721a918d514e0ed60576137ba8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
